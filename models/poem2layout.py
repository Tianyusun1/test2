# models/poem2layout.py (V5.6: ADDED RL SUPPORT)

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel
from .decoder import LayoutDecoder

# === 1. 门控融合模块 (保持不变) ===
class GatedFusion(nn.Module):
    """
    动态融合 Content Features 和 Position Features。
    """
    def __init__(self, hidden_size):
        super().__init__()
        self.gate_net = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        self.norm = nn.LayerNorm(hidden_size)

    def forward(self, content, pos):
        combined = torch.cat([content, pos], dim=-1)
        alpha = self.gate_net(combined)
        fused = alpha * content + (1 - alpha) * pos
        return self.norm(fused)

# === 2. 图神经网络关系先验 (R-GAT) (保持不变) ===
class GraphRelationPriorNet(nn.Module):
    def __init__(self, num_relations, input_dim, hidden_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.num_relations = num_relations
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        assert self.head_dim * num_heads == hidden_dim, "hidden_dim must be divisible by num_heads"

        self.q_proj = nn.Linear(input_dim, hidden_dim)
        self.k_proj = nn.Linear(input_dim, hidden_dim)
        self.v_proj = nn.Linear(input_dim, hidden_dim)
        
        self.rel_embed_k = nn.Embedding(num_relations, hidden_dim)
        self.rel_embed_v = nn.Embedding(num_relations, hidden_dim)

        self.out_proj = nn.Linear(hidden_dim, hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(hidden_dim)
        self.activation = nn.ReLU()

    def forward(self, node_features, spatial_matrix):
        B, T, D = node_features.shape
        H = self.num_heads
        d_k = self.head_dim

        q = self.q_proj(node_features).view(B, T, H, d_k)
        k = self.k_proj(node_features).view(B, T, H, d_k)
        v = self.v_proj(node_features).view(B, T, H, d_k)

        r_k = self.rel_embed_k(spatial_matrix).view(B, T, T, H, d_k)
        r_v = self.rel_embed_v(spatial_matrix).view(B, T, T, H, d_k)

        q = q.unsqueeze(2) 
        k_prime = k.unsqueeze(1) + r_k 
        
        scores = (q * k_prime).sum(dim=-1) / (d_k ** 0.5)
        attn_weights = torch.softmax(scores, dim=2)
        attn_weights = self.dropout(attn_weights)

        v_prime = v.unsqueeze(1) + r_v 
        agg = (attn_weights.unsqueeze(-1) * v_prime).sum(dim=2)
        
        agg = agg.view(B, T, D)
        output = self.out_proj(agg)
        output = output + node_features
        output = self.norm(output)
        output = self.activation(output)
        
        return output

# === 3. 布局变换编码器 (保持不变) ===
class LayoutTransformerEncoder(nn.Module):
    def __init__(self, input_dim=4, hidden_size=768, num_layers=2, nhead=4, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, hidden_size)
        self.pos_embed = nn.Parameter(torch.randn(1, 50, hidden_size))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size, nhead=nhead, dim_feedforward=hidden_size*4, 
            dropout=dropout, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
    def forward(self, boxes, mask=None):
        B, T, _ = boxes.shape
        x = self.input_proj(boxes)
        
        if T <= self.pos_embed.size(1):
            x = x + self.pos_embed[:, :T, :]
        else:
            x = x + self.pos_embed[:, :self.pos_embed.size(1), :]
            
        x = self.transformer(x, src_key_padding_mask=mask)
        
        if mask is not None:
            valid_mask = (~mask).unsqueeze(-1).float()
            sum_feat = (x * valid_mask).sum(dim=1)
            count = valid_mask.sum(dim=1).clamp(min=1e-6)
            global_feat = sum_feat / count
        else:
            global_feat = x.mean(dim=1)
            
        return global_feat

# === 主模型更新 ===
class Poem2LayoutGenerator(nn.Module):
    def __init__(self, bert_path: str, num_classes: int, hidden_size: int = 768, bb_size: int = 64, 
                 decoder_layers: int = 4, decoder_heads: int = 8, dropout: float = 0.1, 
                 reg_loss_weight: float = 1.0, iou_loss_weight: float = 1.0, area_loss_weight: float = 1.0,
                 relation_loss_weight: float = 5.0,  
                 overlap_loss_weight: float = 2.0,   
                 size_loss_weight: float = 2.0,
                 alignment_loss_weight: float = 0.5,
                 balance_loss_weight: float = 0.5,
                 clustering_loss_weight: float = 1.0, 
                 latent_dim: int = 32,               
                 **kwargs): 
        super(Poem2LayoutGenerator, self).__init__()
        
        self.num_element_classes = num_classes 
        self.hidden_size = hidden_size
        self.bb_size = bb_size
        self.latent_dim = latent_dim
        
        # Loss weights
        self.reg_loss_weight = reg_loss_weight
        self.iou_loss_weight = iou_loss_weight
        self.area_loss_weight = area_loss_weight
        self.relation_loss_weight = relation_loss_weight
        self.overlap_loss_weight = overlap_loss_weight
        self.size_loss_weight = size_loss_weight
        self.alignment_loss_weight = alignment_loss_weight
        self.balance_loss_weight = balance_loss_weight
        self.clustering_loss_weight = clustering_loss_weight
        
        self.cond_dropout = nn.Dropout(0.25)

        # 1. Text Encoder
        self.text_encoder = BertModel.from_pretrained(bert_path)
        
        # 2. Object Query Embedding
        self.obj_class_embedding = nn.Embedding(num_classes + 2, bb_size, padding_idx=0)
        
        # 3. Sequence Positional Embedding
        self.seq_pos_embedding = nn.Embedding(50, bb_size)

        # 4. Spatial Bias Embedding
        self.num_spatial_relations = 7
        self.spatial_bias_embedding = nn.Embedding(self.num_spatial_relations, decoder_heads)

        # 5. Position Encoders
        self.grid_encoder = nn.Sequential(
            nn.Linear(64, bb_size), 
            nn.ReLU(),
            nn.Linear(bb_size, bb_size),
            nn.Dropout(dropout)
        )
        
        self.gnn_prior = GraphRelationPriorNet(
            num_relations=self.num_spatial_relations,
            input_dim=bb_size, 
            hidden_dim=bb_size,
            num_heads=4,
            dropout=dropout
        )
        
        self.fusion_gate = GatedFusion(bb_size)

        # 6. CVAE Components
        self.layout_encoder = LayoutTransformerEncoder(
            input_dim=4,
            hidden_size=hidden_size,
            num_layers=2,
            nhead=4,
            dropout=dropout
        )
        self.mu_head = nn.Linear(hidden_size, latent_dim)
        self.logvar_head = nn.Linear(hidden_size, latent_dim)
        self.z_proj = nn.Linear(latent_dim, bb_size)
        
        # 7. KG Projection
        self.kg_projection = nn.Sequential(
            nn.Linear(num_classes, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 8. Decoder
        self.layout_decoder = LayoutDecoder(
            hidden_size=hidden_size,
            bb_size=bb_size,
            num_layers=decoder_layers,
            num_heads=decoder_heads,
            dropout=dropout
        )

        # 9. Head
        decoder_output_size = hidden_size + bb_size
        self.reg_head = nn.Sequential(
            nn.Linear(decoder_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 4)
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def construct_spatial_bias(self, cls_ids, kg_spatial_matrix):
        if kg_spatial_matrix is None:
            return None
        B, S = cls_ids.shape
        map_ids = cls_ids - 2 
        gather_ids = map_ids.clamp(min=0, max=self.num_element_classes - 1)
        b_idx = torch.arange(B, device=cls_ids.device).view(B, 1, 1).expand(-1, S, S)
        row_idx = gather_ids.view(B, S, 1).expand(-1, -1, S)
        col_idx = gather_ids.view(B, 1, S).expand(-1, S, -1)
        rel_ids = kg_spatial_matrix[b_idx, row_idx, col_idx] 
        is_valid_obj = (map_ids >= 0)
        valid_pair_mask = is_valid_obj.unsqueeze(2) & is_valid_obj.unsqueeze(1)
        rel_ids = rel_ids.masked_fill(~valid_pair_mask, 0)
        spatial_bias = self.spatial_bias_embedding(rel_ids) 
        spatial_bias = spatial_bias.permute(0, 3, 1, 2).contiguous() 
        return spatial_bias

    def forward(self, input_ids, attention_mask, kg_class_ids, padding_mask, 
                kg_spatial_matrix=None, location_grids=None, target_boxes=None):
        
        text_encoded = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state 

        content_embed = self.obj_class_embedding(kg_class_ids) 
        content_embed = self.cond_dropout(content_embed)
        
        if target_boxes is not None:
            global_layout_feat = self.layout_encoder(target_boxes, mask=padding_mask) 
            mu = self.mu_head(global_layout_feat)
            logvar = self.logvar_head(global_layout_feat)
            z = self.reparameterize(mu, logvar)
        else:
            B = input_ids.shape[0]
            mu = None
            logvar = None
            z = torch.randn(B, self.latent_dim, device=input_ids.device)
            
        z_feat = self.z_proj(z).unsqueeze(1) 

        pos_feat = torch.zeros_like(content_embed)
        
        if location_grids is not None:
            B, T, H, W = location_grids.shape
            grid_flat = location_grids.view(B, T, -1).to(content_embed.device)
            handcrafted_pos = self.grid_encoder(grid_flat) 
            pos_feat = pos_feat + handcrafted_pos

        if kg_spatial_matrix is not None:
            B, T = kg_class_ids.shape
            # 注意: gnn_prior 内部需要正确处理索引，这里我们传入原始 ids，由 construct_spatial_bias 负责处理
            # 但 GNN 需要的是 spatial_matrix 中提取出的子图
            # 这里复用 construct_spatial_bias 的逻辑来提取 seq_rel_ids
            map_ids = kg_class_ids - 2 
            gather_ids = map_ids.clamp(min=0, max=self.num_element_classes - 1)
            b_idx = torch.arange(B, device=kg_class_ids.device).view(B, 1, 1).expand(-1, T, T)
            row_idx = gather_ids.view(B, T, 1).expand(-1, -1, T)
            col_idx = gather_ids.view(B, 1, T).expand(-1, T, -1)
            seq_rel_ids = kg_spatial_matrix[b_idx, row_idx, col_idx]
            
            learned_pos = self.gnn_prior(content_embed, seq_rel_ids)
            pos_feat = pos_feat + learned_pos 

        pos_feat = self.cond_dropout(pos_feat)

        layout_embed = self.fusion_gate(content_embed, pos_feat)
        
        B, T = kg_class_ids.shape
        seq_ids = torch.arange(T, device=kg_class_ids.device).unsqueeze(0).expand(B, -1)
        seq_embed = self.seq_pos_embedding(seq_ids) 
        layout_embed = layout_embed + seq_embed
        
        layout_embed = layout_embed + z_feat 

        src_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        if padding_mask is not None:
            trg_mask = torch.zeros_like(padding_mask, dtype=torch.float)
            trg_mask = trg_mask.masked_fill(padding_mask, float('-inf'))
            trg_mask = trg_mask.unsqueeze(1).unsqueeze(2)
        else:
            trg_mask = None

        spatial_bias = self.construct_spatial_bias(kg_class_ids, kg_spatial_matrix)

        decoder_output = self.layout_decoder(
            layout_embed, 
            text_encoded, 
            src_mask, 
            trg_mask,
            spatial_bias=spatial_bias
        ) 

        pred_boxes = torch.sigmoid(self.reg_head(decoder_output))
        
        return mu, logvar, pred_boxes, decoder_output

    def forward_rl(self, input_ids, attention_mask, kg_class_ids, padding_mask, 
                   kg_spatial_matrix=None, location_grids=None, target_boxes=None, 
                   sample=True):
        """
        RL 专用的前向传播。
        Treats the network output as the Mean of a Gaussian policy.
        """
        # 1. 复用原有的 forward 获取确定性的预测值 (作为 Gaussian 的均值 mu)
        # 注意：这里我们不需要 target_boxes 来计算 VAE loss，只需生成
        # forward 返回: mu, logvar, pred_boxes, decoder_output
        _, _, pred_boxes_mu, _ = self.forward(
            input_ids, attention_mask, kg_class_ids, padding_mask, 
            kg_spatial_matrix, location_grids, target_boxes=None
        )
        
        # pred_boxes_mu: [B, T, 4] (sigmoid 后的 0-1 值)

        if not sample:
            # Greedy 模式 (Baseline): 直接返回均值，不需要 log_prob
            return pred_boxes_mu, None

        # 2. 构建高斯分布进行采样 (Exploration)
        # 设定一个固定的探索方差，例如 0.1 (可以根据需要调整)
        std = torch.ones_like(pred_boxes_mu) * 0.1
        dist = torch.distributions.Normal(pred_boxes_mu, std)
        
        # 3. 采样动作 (Action)
        action_boxes = dist.sample()
        
        # 4. 截断到 [0, 1] 范围 (因为是 Box 坐标)
        action_boxes = torch.clamp(action_boxes, 0.0, 1.0)
        
        # 5. 计算 Log Probability (用于梯度回传)
        # Sum over the last dimension (x,y,w,h) -> [B, T]
        # 注意: clamp 可能会影响 log_prob 的准确性，但在简单应用中通常忽略边界效应
        log_prob = dist.log_prob(action_boxes).sum(dim=-1)
        
        # Mask 掉 Padding 的部分
        if padding_mask is not None:
             # padding_mask 为 True 的地方 log_prob 设为 0
             log_prob = log_prob.masked_fill(padding_mask, 0.0)

        return action_boxes, log_prob

    def get_loss(self, pred_cls, pred_bbox_ids, pred_boxes, pred_count, layout_seq, layout_mask, num_boxes, target_coords_gt=None, kg_spatial_matrix=None, kg_class_weights=None, kg_class_ids=None):
        loss_mask = layout_mask 
        target_boxes = target_coords_gt
        
        if loss_mask.dim() == 1:
             loss_mask = loss_mask.view(pred_boxes.shape[0], -1)
        
        num_valid = loss_mask.sum().clamp(min=1)

        loss_reg = F.smooth_l1_loss(pred_boxes, target_boxes, reduction='none') 
        loss_reg = (loss_reg.mean(dim=-1) * loss_mask).sum() / num_valid
        
        loss_iou = self._compute_iou_loss(pred_boxes, target_boxes, loss_mask)
        
        pred_w, pred_h = pred_boxes[..., 2], pred_boxes[..., 3]
        tgt_w, tgt_h = target_boxes[..., 2], target_boxes[..., 3]
        pred_area = pred_w * pred_h
        tgt_area = tgt_w * tgt_h
        loss_area = F.smooth_l1_loss(pred_area, tgt_area, reduction='none')
        loss_area = (loss_area * loss_mask).sum() / num_valid
        
        # [FIX] 传入 kg_class_ids 以便正确索引矩阵
        loss_relation = self._compute_relation_loss(pred_boxes, loss_mask, kg_spatial_matrix, kg_class_ids)
        loss_overlap = self._compute_overlap_loss(pred_boxes, loss_mask, kg_spatial_matrix, kg_class_ids)
        
        loss_size_prior = self._compute_size_loss(pred_boxes, loss_mask, num_boxes, kg_class_weights)

        loss_alignment = self._compute_alignment_loss(pred_boxes, loss_mask)
        loss_balance = self._compute_balance_loss(pred_boxes, loss_mask)
        
        loss_clustering = self._compute_clustering_loss(pred_boxes, loss_mask, kg_class_ids)

        total_loss = self.reg_loss_weight * loss_reg + \
                     self.iou_loss_weight * loss_iou + \
                     self.area_loss_weight * loss_area + \
                     self.relation_loss_weight * loss_relation + \
                     self.overlap_loss_weight * loss_overlap + \
                     self.size_loss_weight * loss_size_prior + \
                     self.alignment_loss_weight * loss_alignment + \
                     self.balance_loss_weight * loss_balance + \
                     self.clustering_loss_weight * loss_clustering
                     
        return total_loss, loss_relation, loss_overlap, \
               loss_reg, loss_iou, loss_size_prior, loss_area, \
               loss_alignment, loss_balance, loss_clustering

    def _compute_iou_loss(self, pred, target, mask):
        pred_x1 = pred[..., 0] - pred[..., 2] / 2
        pred_y1 = pred[..., 1] - pred[..., 3] / 2
        pred_x2 = pred[..., 0] + pred[..., 2] / 2
        pred_y2 = pred[..., 1] + pred[..., 3] / 2
        tgt_x1 = target[..., 0] - target[..., 2] / 2
        tgt_y1 = target[..., 1] - target[..., 3] / 2
        tgt_x2 = target[..., 0] + target[..., 2] / 2
        tgt_y2 = target[..., 1] + target[..., 3] / 2
        inter_x1 = torch.max(pred_x1, tgt_x1)
        inter_y1 = torch.max(pred_y1, tgt_y1)
        inter_x2 = torch.min(pred_x2, tgt_x2)
        inter_y2 = torch.min(pred_y2, tgt_y2)
        inter_area = (inter_x2 - inter_x1).clamp(min=0) * (inter_y2 - inter_y1).clamp(min=0)
        pred_area = pred[..., 2] * pred[..., 3]
        tgt_area = target[..., 2] * target[..., 3]
        union_area = pred_area + tgt_area - inter_area
        iou = inter_area / (union_area + 1e-6)
        loss = (1.0 - iou) * mask
        return loss.sum() / (mask.sum().clamp(min=1))

    # [FIXED] Added kg_class_ids and correct indexing
    def _compute_relation_loss(self, pred_boxes, mask, kg_spatial_matrix, kg_class_ids):
        if kg_spatial_matrix is None or kg_class_ids is None:
            return torch.tensor(0.0, device=pred_boxes.device)
        loss = torch.tensor(0.0, device=pred_boxes.device)
        B, S, _ = pred_boxes.shape
        count = 0
        for b in range(B):
            valid_indices = torch.nonzero(mask[b]).squeeze(1)
            if len(valid_indices) < 2: continue
            for i in valid_indices:
                for j in valid_indices:
                    if i == j: continue
                    
                    # [FIX] 使用 kg_class_ids 获取真实的类别 ID
                    cid_i = kg_class_ids[b, i].item()
                    cid_j = kg_class_ids[b, j].item()
                    
                    # [FIX] 映射到 0-8 的矩阵索引
                    idx_i = int(cid_i) - 2
                    idx_j = int(cid_j) - 2
                    
                    # 安全检查：索引必须在有效范围内
                    if not (0 <= idx_i < 9 and 0 <= idx_j < 9):
                        continue
                        
                    rel_id = kg_spatial_matrix[b, idx_i, idx_j].item()
                    if rel_id == 0: continue
                    
                    box_a = pred_boxes[b, i]
                    box_b = pred_boxes[b, j]
                    
                    if rel_id in [1, 5]: # ABOVE / ON_TOP
                        dist = box_a[1] - box_b[1] + 0.05
                        if dist > 0:
                            loss += dist
                            count += 1
                    elif rel_id == 2: # BELOW
                        dist = box_b[1] - box_a[1] + 0.05
                        if dist > 0:
                            loss += dist
                            count += 1
                    elif rel_id == 3: # INSIDE
                        a_x1, a_y1 = box_a[0]-box_a[2]/2, box_a[1]-box_a[3]/2
                        a_x2, a_y2 = box_a[0]+box_a[2]/2, box_a[1]+box_a[3]/2
                        b_x1, b_y1 = box_b[0]-box_b[2]/2, box_b[1]-box_b[3]/2
                        b_x2, b_y2 = box_b[0]+box_b[2]/2, box_b[1]+box_b[3]/2
                        l_inside = F.relu(b_x1 - a_x1) + F.relu(a_x2 - b_x2) + \
                                   F.relu(b_y1 - a_y1) + F.relu(a_y2 - b_y2)
                        if l_inside > 0:
                            loss += l_inside
                            count += 1
        if count > 0:
            return loss / count
        return loss

    # [FIXED] Added kg_class_ids and correct indexing
    def _compute_overlap_loss(self, pred_boxes, mask, kg_spatial_matrix, kg_class_ids):
        loss = torch.tensor(0.0, device=pred_boxes.device)
        B, S, _ = pred_boxes.shape
        count = 0
        for b in range(B):
            valid_indices = torch.nonzero(mask[b]).squeeze(1)
            if len(valid_indices) < 2: continue
            boxes = pred_boxes[b, valid_indices]
            N = boxes.shape[0]
            x1 = boxes[:, 0] - boxes[:, 2]/2
            y1 = boxes[:, 1] - boxes[:, 3]/2
            x2 = boxes[:, 0] + boxes[:, 2]/2
            y2 = boxes[:, 1] + boxes[:, 3]/2
            areas = boxes[:, 2] * boxes[:, 3]
            inter_x1 = torch.max(x1.unsqueeze(1), x1.unsqueeze(0))
            inter_y1 = torch.max(y1.unsqueeze(1), y1.unsqueeze(0))
            inter_x2 = torch.min(x2.unsqueeze(1), x2.unsqueeze(0))
            inter_y2 = torch.min(y2.unsqueeze(1), y2.unsqueeze(0))
            inter_area = (inter_x2 - inter_x1).clamp(min=0) * (inter_y2 - inter_y1).clamp(min=0)
            union_area = areas.unsqueeze(1) + areas.unsqueeze(0) - inter_area
            iou_mat = inter_area / (union_area + 1e-6)
            ignore_overlap = torch.eye(N, device=pred_boxes.device).bool()
            
            if kg_spatial_matrix is not None and kg_class_ids is not None:
                for i_local, i_global in enumerate(valid_indices):
                    for j_local, j_global in enumerate(valid_indices):
                        if i_local == j_local: continue
                        
                        # [FIX] Correct indexing logic
                        cid_i = kg_class_ids[b, i_global].item()
                        cid_j = kg_class_ids[b, j_global].item()
                        idx_i = int(cid_i) - 2
                        idx_j = int(cid_j) - 2
                        
                        if not (0 <= idx_i < 9 and 0 <= idx_j < 9): continue
                        
                        rel = kg_spatial_matrix[b, idx_i, idx_j].item()
                        if rel in [3, 4]: # INSIDE / SURROUNDS -> Allow overlap
                            ignore_overlap[i_local, j_local] = True
                            ignore_overlap[j_local, i_local] = True
                            
            triu_mask = torch.triu(torch.ones(N, N, device=pred_boxes.device), diagonal=1).bool()
            target_mask = triu_mask & (~ignore_overlap)
            if target_mask.sum() > 0:
                bad_iou = iou_mat[target_mask]
                loss += F.relu(bad_iou - 0.1).sum()
                count += target_mask.sum()
        if count > 0:
            return loss / count
        return loss

    def _compute_size_loss(self, pred_boxes, mask, num_boxes, weights=None):
        pred_areas = pred_boxes[..., 2] * pred_boxes[..., 3] 
        if num_boxes is None:
            N = mask.sum(dim=1).clamp(min=1).float() 
        else:
            N = num_boxes.float().clamp(min=1)
        base_expected_area = (0.5 / torch.sqrt(N)).unsqueeze(1) 
        if weights is not None:
            target_areas = base_expected_area * weights
        else:
            target_areas = base_expected_area.expand_as(pred_areas)
        loss = F.smooth_l1_loss(pred_areas, target_areas, reduction='none')
        loss = (loss * mask).sum() / mask.sum().clamp(min=1)
        return loss

    def _compute_alignment_loss(self, pred_boxes, mask):
        B, N, _ = pred_boxes.shape
        loss = torch.tensor(0.0, device=pred_boxes.device)
        count = 0
        for b in range(B):
            valid_indices = torch.nonzero(mask[b]).squeeze(1)
            num_valid = len(valid_indices)
            if num_valid < 2: continue
            boxes = pred_boxes[b, valid_indices]
            cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
            left = cx - w / 2
            right = cx + w / 2
            top = cy - h / 2
            bottom = cy + h / 2
            x_vals = torch.stack([left, cx, right], dim=1) 
            y_vals = torch.stack([top, cy, bottom], dim=1)
            
            def min_dist_loss(vals):
                v1 = vals.unsqueeze(1) 
                v2 = vals.unsqueeze(0) 
                diff = torch.abs(v1.unsqueeze(3) - v2.unsqueeze(2)).view(num_valid, num_valid, -1)
                eye_mask = torch.eye(num_valid, device=vals.device).bool().unsqueeze(-1)
                diff = diff.masked_fill(eye_mask, 100.0)
                min_dists, _ = diff.min(dim=2) 
                min_dists, _ = min_dists.min(dim=1) 
                return min_dists.mean()

            loss += min_dist_loss(x_vals) + min_dist_loss(y_vals)
            count += 1
        if count > 0:
            return loss / count
        return loss

    def _compute_balance_loss(self, pred_boxes, mask):
        B, N, _ = pred_boxes.shape
        loss = torch.tensor(0.0, device=pred_boxes.device)
        count = 0
        target_center = 0.5
        margin = 0.15 
        
        for b in range(B):
            valid_indices = torch.nonzero(mask[b]).squeeze(1)
            if len(valid_indices) == 0: continue
            
            boxes = pred_boxes[b, valid_indices]
            cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
            
            areas = w * h
            total_area = areas.sum().clamp(min=1e-6)
            
            center_x = (cx * areas).sum() / total_area
            center_y = (cy * areas).sum() / total_area
            
            dist_x = F.relu(torch.abs(center_x - target_center) - margin)
            dist_y = F.relu(torch.abs(center_y - target_center) - margin)
            
            loss += (dist_x + dist_y)
            count += 1
            
        if count > 0:
            return loss / count
        return loss

    def _compute_clustering_loss(self, pred_boxes, mask, kg_class_ids):
        if kg_class_ids is None:
            return torch.tensor(0.0, device=pred_boxes.device)
            
        loss = torch.tensor(0.0, device=pred_boxes.device)
        B, N, _ = pred_boxes.shape
        count = 0
        max_dist_threshold = 0.35 
        
        for b in range(B):
            classes = kg_class_ids[b]
            valid_mask = mask[b]
            unique_classes = torch.unique(classes)
            
            for cls_id in unique_classes:
                if cls_id <= 2: continue 
                
                indices = torch.nonzero((classes == cls_id) & (valid_mask > 0)).squeeze(1)
                
                if len(indices) < 2: continue 
                
                for i in range(len(indices)):
                    for j in range(i + 1, len(indices)):
                        idx1 = indices[i]
                        idx2 = indices[j]
                        
                        box1 = pred_boxes[b, idx1] 
                        box2 = pred_boxes[b, idx2]
                        
                        dist = torch.sqrt((box1[0] - box2[0])**2 + (box1[1] - box2[1])**2)
                        
                        if dist > max_dist_threshold:
                            loss += (dist - max_dist_threshold)
                            count += 1
                            
        if count > 0:
            return loss / count
        return loss